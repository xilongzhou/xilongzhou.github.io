<!DOCTYPE html>
<html data-wf-site="554481fae11f375d20614d0e" data-wf-page="554481fae11f375d20614d10" data-wf-status="1" class="w-mod-js w-mod-no-touch w-mod-video w-mod-no-ios wf-lato-n1-active wf-lato-i1-active wf-lato-n3-active wf-lato-i3-active wf-lato-n4-active wf-lato-i4-active wf-lato-n7-active wf-lato-i7-active wf-lato-n9-active wf-lato-i9-active wf-roboto-n3-active wf-roboto-n4-active wf-roboto-n5-active wf-robotocondensed-n3-active wf-robotocondensed-n4-active wf-robotocondensed-n7-active wf-robotoslab-n3-active wf-robotoslab-n4-active wf-robotoslab-n7-active wf-arbutusslab-n4-active wf-active"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>zhouyi</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="generator" content="Webflow"><link rel="stylesheet" type="text/css" href="./zhouyi_files/zhouyi.webflow.9320a162c.css"><script src="./zhouyi_files/webfont.js"></script><link rel="stylesheet" href="./zhouyi_files/css"><script>WebFont.load({
  google: {
    families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic","Roboto:300,regular,500","Roboto Condensed:300,regular,700","Roboto Slab:300,regular,700","Arbutus Slab:regular"]
  }
});</script><script type="text/javascript" src="./zhouyi_files/modernizr-2.7.1.js"></script><link rel="shortcut icon" type="image/x-icon" href="https://daks2k3a4ib2z.cloudfront.net/537a7966446efcce2d6dcce5/537d2eeceffa99671899c195_New%20icon.ico"><link rel="apple-touch-icon" href="https://daks2k3a4ib2z.cloudfront.net/537a7966446efcce2d6dcce5/537d2ecdeffa99671899c192_metric-webclip.png"></head>
<body>
<div data-collapse="medium" data-animation="default" data-duration="400" data-contain="1" class="w-nav navigation">
	<div class="w-container">
		<nav role="navigation" class="w-nav-menu nav-menu">
			<a href="./index.html#top" class="w-nav-link nav-link w--current" style="max-width: 940px;">About</a>
			<a href="./index.html#research" class="w-nav-link nav-link" style="max-width: 940px;">Research</a>
			<a href="./index.html#project" class="w-nav-link nav-link" style="max-width: 940px;">Project</a>
		</nav>
	<div class="w-nav-button nav-link menu"><div class="w-icon-nav-menu"></div></div>
	</div>
	<div class="w-nav-overlay" data-wf-ignore=""></div>
</div>

<div id="top" class="w-section section main">
	<div class="w-container selintro-container" >
		<div class="w-row">
			<div class="w-col w-col-5">
				<img width="306" src="./zhouyi_files/zy_laopera.png">
			</div>
			<div class="w-col w-col-7">
					<div class="w-col w-col-8">
						<div><h1><font color=white> Yi Zhou</font></h1></div>
						<div class="email-text"><font color=white>zhou859@usc.edu</font>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="cv_zhouyi.pdf" target="_blank"><font color=white>CV</font></a></div>
					</div>
					<div class="w-col w-col-2">
						<div style="width:100px"><script type="text/javascript" id="clstr_globe" src="https://cdn.clustrmaps.com/globe.js?d=6Hgf3YMgOYj6NWfc29ulxldS2RV6PES-R2sn7qRa9Jc"></script>
						</div>
					</div>
					
					<div><pw><br><br><br><br><br><br>I am a Ph.D. candidate at University of Sourthern California under the supervision of Dr. Hao Li. My researches lie in finding deep representations for complex graphics and vision problems and creating virtual humans. In the spare time, I love singing, dancing, painting and playing the Ukulele.</pw></div>
			</div>
			
		</div>
	</div>
</div>



<div id="research" class="w-section section press" >
	<div class="project-contatiner w-container"><h1 class="main-heading">Research</h1>
	<hr class="myhr" ></hr>
	
	<div class="project-contatiner w-container">
		<h1 class="project-heading" >Autonomous 3D Avatars</h1>
		<p>This project is about making 3D Avatars move automatically. The topics that I have been looking into are complex human motion synthesis and motion planning using deep learning and the continuity problem in the motion representations in deep neural networks. </p>
		
		<p><b>Publications:</b></p>
		
		<p>ON THE CONTINUITY OF ROTATION REPRESENTATIONS IN NEURAL NETWORKS
		<br><b>Yi Zhou*</b>, Connelly Barnes*, Jingwan Lu, Jimei Yang, Hao Li
		<br><em>CVPR 2019</em> [<a href="https://arxiv.org/pdf/1812.07035.pdf">paper</a>]  [<a href="project_rotation/rotation.html">project page</a>] 
		
		<p>AUTO-CONDITIONED LSTM NETWORK FOR EXTENDED COMPLEX HUMAN MOTION SYNTHESIS
		<br><b>Yi Zhou*</b>, Zimo Li*, Shuangjio Xiao, Chong He, Zeng Huang, Hao Li
		<br><em>ICLR 2018</em>
		[<a href="https://arxiv.org/pdf/1707.05363.pdf">paper</a>]
		[<a href="https://youtu.be/zI1HOyruYcY">video</a>]</p>
		<br>
		<iframe width="533" height = "300"  src="https://www.youtube.com/embed/AWlpNeOzMig?rel=0&amp;controls=1&amp;showinfo=0;loop=1;autoplay=1" frameborder="0"  allowfullscreen></iframe>
	</div>
	
	
	<div><br></div>
	<hr class="myhr" ></hr>
	
	<div class="project-contatiner w-container">
		<h1 class="project-heading" >Hair</h1>
		<p>The project is about finding the deep representation for 3D hair geometries and automatically tracking and reconstructing human hairs from single or multi-view cameras. We designed the first deep neural neural network that can fastly infer high-resolution 3D hairs. We also provide an open-source [<a href="https://github.com/papagina/HairNet_DataSetGeneration">dataset</a>] that contains 40 thousand synthetic hair models. </p>
		
		<p><b>Publication:</b></p>
		
		<p>SINGLE-VIEW HAIR RECONSTRUCTION USING CONVOLUTIONAL NEURAL NETWORKS
		<br><b>Yi Zhou</b>, Liwen Hu, Jun Xing, Weikai Chen, Han-Wei Kung, Xin Tong, Hao Li
		<br><em>ECCV 2018</em> [<a href="https://arxiv.org/pdf/1806.07467.pdf">paper</a>] [<a href="https://youtu.be/MLnS-gTWc9w">video</a>]</p>
		<br>
		<img src="./zhouyi_files/hair.jpg" />
		
	</div>
	
	<div><br></div>
	<hr class="myhr"></hr>
	
	<div class="project-contatiner w-container">
		<h1 class="project-heading" >Face</h1>
		<p>The project is about Track the expressions and textures of the face from videos, then synthesize new facial images and video with given expressions, e.g. swap the face between two videos. </p>
		
		<p><b>Publication:</b></p>
		
		<p>REALISTIC DYNAMIC FACIAL TEXTURES FROM A SINGLE IMAGE USING GANS
		<br>Kyle Olszewski, Zimo Li, Chao Yang, <b>Yi Zhou</b>, Ronald Yu, Zeng Huang, Sitao Xiang, Shunsuke Saito, Pushmeet Kohli, Hao Li
		<br><em>ICCV 2017</em> [<a href="http://www.hao-li.com/publications/papers/iccv2017RDFTFSIUG.pdf">paper</a>] [<a href="https://youtu.be/R-kkq4xxVNw">video</a>] [<a href="http://www.hao-li.com/publications/additionalMaterials/iccv2017additionalMaterialsB.pdf">additional materials</a>]</p>
		<br>
		<img src="./zhouyi_files/123faceICCV.png" width="400"/>
		
	</div>
	
	<div><br></div>
	<hr class="myhr"></hr>

	<div class="project-contatiner w-container">
		<h1 class="project-heading" >Augmented Reality</h1>
		<p>Inspired by the hologram system in manga "Psycho Pass" I design the Pmomo (projection mapping on movable object) system to create the phantasm of real-world objects being covered with virtual exteriors. As supporting 6-DOF object motion, the system can keep tracking the object and projecting 3D texture on its surface in real-time. Meanwhile, occlusions are culled from projection as to improve the sense of realism. In the picture left, the models held in the user' hands were originally white but now rendered with vivid textures by projection. </p>
		
		<p>For watching the demonstration videos: [<a href="https://youtu.be/K3KLlAA4pqY" target="_blank">preview</a>] [<a href="https://youtu.be/duCLF4rbYDc" target="_blank">surprise</a>] [<a href="https://youtu.be/4W3z9JyKmt4">demo</a>]</p>
		
		<p><b>Publication:</b></p>
		
		<p>PMOMO: PROJECTION MAPPING ON MOVABLE 3D OBJECT
		<br><b>Yi Zhou</b>, Shuangjiu Xiao, Ning Tang, Zhiyong Wei, Xu Chen
		<br><em>CHI 2016</em> [<a href="https://dl.acm.org/citation.cfm?id=2858329">paper</a>]</p>
		<br>
		<img src="./zhouyi_files/pmomo.jpg" />
			
		
	</div>
	
	<div><br></div>
	
	<hr class="myhr"></hr>
	
	<div><br></div>

</div>

<div id="project" class="w-section section main" >
	<div class="w-container"><h1 class="main-heading"  style="color:white">Project</h1></div>
	
	<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><div data-animation="slide" data-duration="500" data-infinite="1" class="w-slider"><div class="w-slider-mask"><div class="w-slide" style="transform: translateX(0px); opacity: 1;"><img src="./zhouyi_files/5667bbfb53f0878447273aa5_getandput1.jpg"></div><div class="slider-2 w-slide" style="transform: translateX(0px); opacity: 1;"><img src="./zhouyi_files/getandput.png"></div><div class="slider-2 w-slide" style="transform: translateX(0px); opacity: 1;"><img src="./zhouyi_files/getandput4.png"></div><div class="w-slide" style="transform: translateX(0px); opacity: 1;"><img src="./zhouyi_files/5667bc1056cef67b477bf6a1_getandput2.jpg"></div></div><div class="w-slider-arrow-left"><div class="w-icon-slider-left"></div></div><div class="w-slider-arrow-right"><div class="w-icon-slider-right"></div></div><div class="w-slider-nav w-round"><div class="w-slider-dot w-active" data-wf-ignore=""></div><div class="w-slider-dot" data-wf-ignore=""></div></div></div></div>
	<div class="w-col w-col-7">
	<h1 class="project-heading">Get&amp;Put</h1><p>It is a cloud powered magic that enable users to grab pictures and music directly from the screen, carrying around the data and put them 'into' other smart devices. Through hand commands, the files are transferred from device-to-device without any extra equipment. For watching the videos:
	[<a href="http://youtu.be/y-kg4rBbSgU">fun</a>] [<a href="http://youtu.be/gpjOyr89q5Y">child</a>] [<a href="http://youtu.be/bm1NE5nQNyc">demo</a>]</p>
	<p>It won the <b> Global 3rd place of 2013 Imagine Cup - Azure Challenge</b> and the <b>3rd place of 2013 Imagine Cup - Mail. Ru Awards</b></p></div>
	</div></div>



	<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><img src="./zhouyi_files/5667b74dedabe1ea47f18f28_clio1.jpg"></div>
	<div class="w-col w-col-7">
	<h1 class="project-heading">Clio Super Painter</h1>
	<p>Clio is an android app that makes painting a new way of communication. It allows users to do synchronous painting and picture transmission on more than three Android devices via Wi-Fi and Bluetooth.&nbsp;To make it more fun, Clio also generates special effects when 'telepathy' happens between the users.<br>For downloading the apps and watching the videos: 
	[<a href="https://www.youtube.com/watch?v=Fbb_CXxDECU">short</a>] [<a href="https://www.youtube.com/watch?v=Zxa9EKbRirc">fun</a>]</p>
	<p>It won the <b>Global 1st place of 2012 Ericsson Application Awards<b><p></div></div></div>


	<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><img src="./zhouyi_files/5667ba7053f0878447273a7f_motion1.jpg"></div><div class="w-col w-col-7"><h1 class="project-heading">Human Motion Recognition Based on Joint Motion Image</h1><p>In the field of human motion recognition methods, the objective of this research is to transform motions into images and accurately recognize the motion images, under very small training sets.</p></div></div></div>

	<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><img width="361" src="./zhouyi_files/554614efabdddf0146915c01_spongebob.png"></div><div class="w-col w-col-7"><h1 class="project-heading">Performance-based 3D Cartoon Facial Animation Control</h1><p>I developed this software to create a windows application that can make the avatars follow the facial and body actions of the users in real time. In the left picture, I am controlling 'Sponge Bob' to perform funny actions of nodding and laughing. My method is building several key facial expression models, and morphing them into a new expression according to the facial coefficients tracked by Kinect.</p></div></div></div>

	<div class="w-container project-contatiner"><div class="w-row project-row"><div class="w-col w-col-5"><img width="375" src="./zhouyi_files/5546169eabdddf0146915c14_avat.png"></div><div class="w-col w-col-7"><h1 class="project-heading">AVAT: Automatic Visual Activity Test</h1><p>AVAT is a medical application based on Kinect, and it can offer multiple self-catering vision test items, including: visual acuity test, chromatoptometry test and stereopsis vision test. It is very easy and convenient for hospital and home usage. Plus, AVAT can track the vision test results, helping the specialists to assess the change of visual condition, e.g. parents willing to monitor the vision acuity of their children. With the popularization of the AVAT system in medical institutions, schools and communities, it could solve the issue&nbsp; of the overloaded physical examinations in China.</p><div class="highlight-text">It won the Chinese local 2rd prize of 2012 Imagine Cup&nbsp;</div></div></div></div>
</div>
</div>

<div class="w-section section footer copyright"><div class="w-container"><div>COPYRIGHT 2015. ALL RIGHTS RESERVED</div></div></div>
<script type="text/javascript" src="./zhouyi_files/jquery.min.js"></script>
<script type="text/javascript" src="./zhouyi_files/webflow.72472ab46.js"></script>
<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->